<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>reveal-md</title>
    <link rel="stylesheet" href="./css/reveal.css" />
    <link rel="stylesheet" href="./css/theme/simple.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="./_assets/additional.css" />

  </head>
  <body>
    <div class="navi">
        <p id='lc'>Language Contact</p>
        <p id='calc'>CALC</p>
        <p id='cs'>Case Study</p>
        <p id='results'>Results</p>
        <p id='outlook'>Outlook</p>
    </div>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template"><h4>STUDYING LANGUAGE CONTACT IN SOUTH EAST ASIA WITH THE HELP OF COMPUTER-ASSISTED APPROACHES</h4>
<p><img src='img/calc-yinyang.png' style="border: 0.1;border-color: white;width:25%"></img></p>

<p style='font-size:30px; text-align:center;'>International Conference on Historical Linguistics #24</p>
<p style='font-size:20px; text-align:center;'>2019.07.02</p></script></section><section ><section data-markdown><script type="text/template"><div class='lc'></div>
<h3>South East Asia linguistic area</h3>
<div class="fig-container" style="overflow:hidden;"
        data-overflow-shown=true
        data-file="interactive/point_map_Eurasia_color.html"
        data-style="height: 600px; margin-top: -100px; width: 100%;">


<aside class="notes"><p>There are about 7000 languages in the world, as we can see, languages are densely distributed in South East Asia, Africa and South America. Noticeably, two of the three major language families lie in South East Asia, namely, the Sino-Tibetan language family and the Austronesian language family. In addition, there are three smaller language families, in terms of the number of variants, spoken within the range of these two major language families, the Austro-asiatic, the Hmong-Mien and the Tai-Kadai language family.
The speakers of the languages of the Sino-Tibetan, Hmong-Mien and Tai-kadai language families have long term co-habitat history, and thus language contact should be expected. . Green is ST, blue is HM, bright yellow is Austronesian, purple is Austroasiatic, orange is TK</p>
</aside></script></section><section data-markdown><script type="text/template"><p style='font-size:30px;'> Chinese loanwords are found in Hmong-Mien and Tai-Kadai language families</p>
<div id='leftequal'>
<img src='img/Zeng_KT_language_contact.png' style='border:0px; width:100%'></img>
<p style='font-size:15px;'>(Zeng, 2010)</p>
</div>
<div id='rightequal'>
<img src='img/Ratliff_HM_language_contact.png' style='border:0px; width:90%'></img>
<p style='font-size:15px;'>(Ratliff, 2010)</p>
</div>
<aside class="notes"><p>(Lexical borrowing) Long term co-habitation leads to inevitable language contacts. One of the most significant features of proving language contacts is lexical borrowing. Both (Ratliff, 2010) and (Zeng, 2010) pointed out that a lot of Chinese loanwords are found in HM and TK languages</p>
</aside></script></section><section data-markdown><script type="text/template"><p style='font-size:30px;'>A mechanism to shape languages in the world </p>
<div id='leftequal'>
<img src='img/Lyle_Campbell_bookcover.png' style='border:0px; width:90%'></img>
</div>
<div id='rightequal'>
<blockquote style='font-size:30px;color:#767e87;text-align:left;width:100%'> The book, <i>Historical Linguistics</i> (Campbell, 1998) summarizes three major mechanisms of language change: regular sound change, analogy, and <b>lexical borrowing.</b></blockquote>
</div>

<aside class="notes"><p>(Lexical borrowing) language contact is thought to be one of the mechanisms to shape modern languages.</p>
</aside></script></section><section data-markdown><script type="text/template"><p style="font-size:40px;text-align:left;">Studying lexical borrowing helps to </p>
  <ul>
    <li>resolve the issues of identifying cognates and loanwords so as to gain insight into the relationship among languages in the area. </li>
    <li>reveal the diachronic evolution of target languages.</li>
    <li>reconstruct the history of language contacts and population migrations. </li>
  </ul>


<aside class="notes"><p>Especially, in the area where speakers of various language families co-inhabit for a long time, like South East Asia, language contacts are expected to play an important role in language changes. So investigating lexical borrowings in South East Asian languages helps to shed light on the history of language changes in this area.
In addition, speakers of Hmong-Mien languages have a long migration history, the speakers are nowadays widely scattered in China and the neighboring countries. Identifying the loanwords and the source of borrowings, might help to infer the migration routes of the populations in the past. Therefore, studying lexical borrowings also provides the information in reconstructing human migration patterns in South East Asia.</p>
</aside></script></section></section><section ><section data-markdown><script type="text/template"><p style="font-size:40px;text-align:left;">The classical method - 关系词分层法 </p>

<p style="font-size:30px;text-align:left;">关系词分层法 (guān xì cí fèn céng fǎ) is the stratification of related words</p>

<p style="font-size:30px;text-align:left;">The <b>related words</b> represent lexical items that show sound and semantic correspondences among languages.</p>
<div id='leftequal'>
<img src='img/related_words.png' style='border:0px;width:100%'></img>

<p style='font-size:10px'>(Zeng, 2004)</p>
</div>
<div id='rightequal'>
<img src='img/layers_of_loans.png' style='border:0px;width:80%'></img>

</div>
<aside class="notes"><p>This method was inspired by Sagart&#39;s method of the stratification of Chinese loanwords into Dazhai Hani (@Sagart2001). The idea of stratification of Chinese loanwords is that the loans which were borrowed from a donor language into a recipient language might come in different period of time, When the loans accumulated in the recipient language to a certain amount, these loans form identifiable patterns of correspondence with the donor Chinese dialect, which is called loan correspondence (@Sagart2001). Detecting the loan correspondence relies on the <em>Principle of Coherence</em> -- the initial, rhyme and tone correspondences on a borrowed syllabic morpheme obey the same set of correspondences(@Sagart2001). This rule applies to di- syllabic words or phrases, too. The method of stratification of related words divides the loanwords into 4 time periods in the order of top (Modern borrowing) to bottom (ancient borrowing). The purpose is to find the regular sound correspondence sets in each history layer and use this information as well as other linguistic features to separate loanwords from cognates (@Zeng2004, @Zeng2010).</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>The disadvantage of the classical method</h3>

<ul>
  <li> The research progress is slow. </li>
  <li>The classical approach focuses on areal contacts.</li>
  <li> It is hard to imagine the <b>intensity</b> of language contacts with the traditional presentations.</b></li>
</ul>

<aside class="notes"><p>The traditional method relies on manually inspecting a huge amount of lexical items, it is time consuming. Also, the traditional methods can only study few languages at a time, and linguists often sample languages which are spoken in a relatively small area. Various small scale studies results in scattered research outcomes, putting pieces of information together into a clear picture is also a time consuming process, not to mention, if long range population migration was involved, then studying only areal contacts cannot identify the long distance language contacts in the past.
After several years of studies, we aware of the fact that language contacts occur frequently in South East Asia. But the language contact patterns, for example, shared concepts and provider-recipient pairs remain unclear.</p>
</aside></script></section><section data-markdown><script type="text/template"><div class='calc'></div>
<h3>Automatic methods</h3>
<ul>
  <li>Phylogeny-related conflicts (MLN approach, Nelson-Sathi et al.
2011, List et al. 2014)</li>
  <li>Similar words related across unrelated languages (Mennecier et al. 2016)</li>
  <li>Distribution-related conflicts (as proposed by Sergey Yakhontov, as
reported by Starostin 1990, Chén 1996, or McMahon et al. 2005)</li>
  <li> <b>Distance-based method</b> (SplitsTree,  Huson et al., 2006)</li>
</ul>

<aside class="notes"><p>Computer programs can help with the first issues. Computer programs can identify the loanwords by finding the conflicting signals. Scholars can harvest the computational loanwords quicker than using traditional method. Due to the time limit, I will skip introducing the methods here.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>Tasks in order to study language contacts</h3>
<ul>
  <li>Compile a new dataset </li>
    <ul>
      <li> More than 20 languages </li>
      <li> More than 2 language families</li>
    </ul>
  <li>Visualize language contact patterns</li>
    <ul>
      <li> Between language pairs</li>
      <li> Intensity</li>
    </ul>
</ul>
<aside class="notes"><p>Surveying the traditional method, I find that there are two important tasks to do in order to improve the existing methods to study language contacts. First, compile a new dataset with more than 20 languages, in fact, as many as possible. And more than 2 language families. Second, the way to visualize language contact patterns is needed. Essentially, a framework which is efficient, consistent and easy to inspect.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>Language-contact Workflows</h3>
<p style="text-align:center">
<img src="http://lingulist.de/documents/talks/img/edictor-tutorial/calc-7.png" alt="img" style="width:1000px;text-align:center;"></img>
</p>

<aside class="notes"><p>The computer-assisted approach can achieve the criteria I mentioned in the previous slide. As we all know that computer programs are efficient and consistent. However, there is no perfect computer algorithm can replace linguists&#39; knowledge. Sometimes, we need to rely on linguists&#39; intuition and experiences, but we don&#39;t want to end up having results which are not reproducible. So the combination of computer programs and experts&#39; knowledge allows us to enjoy the best of two worlds, the efficiency and consistency from the computing power and the flexibility and accuracy from the experts&#39; judgements.</p>
</aside></script></section><section data-markdown><script type="text/template"><img src='img/contact_workflow.png' style='border:0px;width:80%;'></img>


<aside class="notes"><p>This is an on-going work, the dashed line indicates the planned tasks. The workflows combines experts&#39; inputs and computer algorithms to take care of the repetitive or time consuming tasks.
Today, in the results section, I will focus on presenting the works which I have down.</p>
<p>The workflows borrowing detection workflows including three major components</p>
<ol>
<li>prepare data for automatic processing (LIFT)<ul>
<li>map elicitation glosses to Concepticon (List et al. 2016)</li>
<li>link languages to Glottolog (Hammarstroem et al. 2018)</li>
<li>clean lexical entries</li>
<li>convert forms to segmented IPA transcriptions</li>
</ul>
</li>
<li>identify cognate sets<ul>
<li>identify cognates among all languages for varying thresholds using LingPy&#39;s methods (List et al. 2018)</li>
<li>use methods that account for sound correspondences and methods that identify shallow similarities</li>
<li>follow the principle of Mennecier, by assuming that words detected by SCA among unrelated languages are likely borrowings</li>
</ul>
</li>
<li>analyze borrowings<ul>
<li>interactive heatmaps for manual inspection</li>
<li>count frequency of borrowed pairs and calculate correlations with WOLD</li>
<li>construct split trees</li>
<li>construct contact networks</li>
</ul>
</li>
</ol>
</aside></script></section><section data-markdown><script type="text/template"><h4>Compile a new dataset - obstacles</h4>
<div id='left'>
<img src='img/Lexicon_layout.png' style='border:0px;'></img>
</div>
<div id='right'>
<ul>
  <li style='font-size:30px'>Inconsistent data format</li>
  <li style='font-size:30px'>Non-uniformed annotation</li>
</ul>
</div>
<aside class="notes"><p>The issues of working with raw data are listed in this slide. As you can see the examples at the left hand side, the data formats are inconsistent. The Annotation is not unformed, either. The annotation including the grapheme usages, glosses and the way to express synonyms.</p>
</aside></script></section><section data-markdown><script type="text/template"><h4>Compile a new dataset - CALC solution </h4>
<img src='img/Raw_to_Machine.png' style='border:0px;width:80%;'></img>

<aside class="notes"><p>The solution here is to standardize the data format. The approach is that the glosses might be annotated in various ways but in the same meaning, then the glosses can be linked to the same concept set and also assigned a unique identification number. Later, the data can be pooled together according to the concept sets. The grapheme usages can also be normalized by using orthography profiles to convert to international phonetic alphabets (IPA). The profile can also help with segmentations. And lastely, the synonyms are separated into different entires, so a row contains only one reflex.</p>
</aside></script></section><section data-markdown><script type="text/template"><h4>Cognate judgement</h4>
<p style='font-size:30px;'>LingPy: A Python Library for Historical Linguistics</p>
<div class="fig-container" style="overflow:hidden;"
        data-overflow-shown=true
        data-file="http://lingpy.org/"
        data-style="height: 550px; margin-top: -80px; width: 100%;">
</div>

<aside class="notes"><p>For cognate detections, we use LingPy, a python library for historical linguistics. It provides two algorithms to detect congates, one is SCA and another one is LexStat. Both algorithms rely on aligning phonetic sequences, estimate the similarities and use a given threshold to cluster lexical items into cognate sets. But the difference is that LexStat employs a striker criteria to identify cognates. SCA method measures the surface similarities among reflexes, and it is suspected SCA method can help to identify loanwords.</p>
</aside></script></section></section><section ><section data-markdown><script type="text/template"><div class='cs'></div>

The selected lexical items with corresponding language family material were drawn from the following studies:

<img src='img/Overview_data_table.png' style='border:0px;width:60%'></img>
</script></section><section data-markdown><script type="text/template"><ul>
 <li> **Convert lexical data** of individual studies to machine-readable formats </li>
    <ul>
    <li>Link the concept to Concepticon database</li>
    <li>Each entry contains only one value </li>
    <li>Assign Glottolog codes to language names</li>
    </ul>
 <li> The compiled cross-linguistic data consists of 48 languages. </li>
    <ul>
      <li>Subset of lexical data of languages from datasets</li>
      <li>Calculate the mutual coverage between each language pair.</li>
    </ul>
</ul>
</script></section><section data-markdown><script type="text/template"><h3>Analysis</h3>

<ul>
  <li>Determine the cognates sets by using LexStat and SCA methods.</li>
  <li>Calculate pairwise language distances via LexStat and SCA cognates.</li>
  <li>Construct Neighbor-nets from the language distances.</li>
  <li>Produce interactive heatmaps to inspect the language distances with various clustering thresholds.</li>
</ul>

<aside class="notes"><p>This is an ongoing work, so I will present the plan we have so far.</p>
</aside></script></section></section><section ><section data-markdown><script type="text/template"><div class='results'></div>


| dataset     |   height |   width |   words |   coverage |
|:------------|---------:|--------:|--------:|-----------:|
| Castro, 2015|      592 |      16 |    9693 |       0.98 |
| Chen, 2013  |      883 |      25 |   21617 |       0.95 |
| Beida, 1964 |      905 |      18 |   18069 |       1    |
| Allen, 2007 |      499 |       9 |    4546 |       1    |
| Sun, 1991   |      996 |      51 |   50446 |       0.92 |
| all-data    |     1480 |      46 |   35203 |       0.32 |

<aside class="notes"><p>As it is shown that the mutual coverage rate is low in the end, but we can still have about 300 concepts to test the workflows.</p>
</aside></script></section><section data-markdown><script type="text/template"><h4>Splits Tree - LexStat (threshold 60)</h4>
<img src='img/lexstat_60_im_id_distances_dst.png' style='border:0px;width:80%'></img>

<aside class="notes"><p>The web-like structure is shown.</p>
</aside></script></section><section data-markdown><script type="text/template"><h4>Splits Tree - SCA (threshold 40)</h4>
<img src='img/sca_40_im_id_distances_dst.png' style='border:0px;width:80%'></img>

<aside class="notes"><p>The web-like structure is larger than LexStat. However, it is hard to find which glosses create such differences. Also, the conflicting signals might be created due to several reasons. At this moment, it is hard to arrive a conclusion that the web-like structure is due to only loanwords.</p>
</aside></script></section><section data-markdown><script type="text/template"><p style='font-size:30px'>Interactive heatmap</p>

<div class="fig-container" style="overflow:hidden;"
        data-overflow-shown=true
        data-file="interactive/contact.html"
        data-style="height: 600px; margin-top: -20px; width: 100%;">
</div>

<aside class="notes"><p>We visualize the pairwise distance by using heatmap. Darker colors represent the languages distances are smaller. An interative heatmap shows that the language contacts occur frequently on :
(1) HM-Sinitic pairs
(2) HM-Sui pairs</p>
<p>However, Sui languages in this graph seem don&#39;t have frequent contacts with Sinitic languages. No cognate is shared between Sui languages and Sinitic languages cannot be seen as a direct evidence that Sui languages do not have contact with Sinitic languages. It only indicate that Sui languages have less interaction with the Sinitic languages we selected. In fact, there are books about Chinese loanwords in Sui languages, but the dialects do not digitised data for this experiment.</p>
</aside></script></section></section><section ><section data-markdown><script type="text/template"><div class='outlook'></div>

<img src='img/outlook.png' style='border:0px;'></img></script></section><section data-markdown><script type="text/template"><h3>Discussion</h3>
<ul>
<li>Conflicting signals come from various sources</li>
  <ul>
    <li>Data noise</li>
    <li>Lexical borrowings</li>
    <li>False positive cognate judgement errors</li>
  </ul>
<li>What words were actually borrowed cannot be shown by SplitsTree</li>
</ul>
<aside class="notes"><p>Overall, the task 2 is not yet finished.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>Future work - I </h3>
<ul style='font-size:30px;'>
 <li>Infer loanwords from SCA and LexStat cognates</li>
 <li>Count the frequency of borrowed pairs per concept and compare with WOLD</li>
</ul></script></section><section data-markdown><script type="text/template"><h3>Future work - II </h3>
<ul style='font-size:30px;'>
 <li>Modeling the borrowing pattern</li>
  <ul style='font-size:20px;'>
    <li>The donor-recipient language pairs. </li>
    <li>Language contact intensity.</li>
  </ul>
</ul>

<img src='img/Delaunay_triangle.png' style='border:0px;width:50%;'></img>
</script></section></section><section ><section data-markdown><script type="text/template"><h3> Reference </h3>
<ul style='font-size:20px'>
<li> Campbell (2013). Historical linguistics : an introduction.</li>
<li> Zeng (2004). Han yu Shui yu guan xi lun : Shui yu li Han yu jie ci ji tong yuan ci fen ceng yan jiu.</li>
<li> Zeng (2010). The study of Chinese loanwords in Kam-Tai and Miao-Yao languages.</li>
<li> Sagart(2001).History through loanwords : the loan correspondences between Hani and Chinese.</li>
<li> Ratliff (2010).Hmong-Mien language history.</li>
<li> List (2014).Automated methods for the investigation of language contact, with a focus on lexical borrowing. Language and Linguistics Compass.</li>
<li> List (2016i). Computer-Assisted Language Comparison: Reconciling Computational and Classical Approaches in Historical Linguistics.</li>
<li> List (2017). LingPy. A Python library for quantitative tasks in historical linguistics.</li>
<li> List (2012). LexStat. Automatic detection of cognates in multilingual wordlists.</li>
<li> Mennecier (2016). A Central Asian Language Survey.</li>
<li> Huson (2008). SplitsTree 4.0-Computation of phylogenetic trees and networks.</li>
<li> Chen (2013). Miàoyáo yǔwén.</li>
<li> Castro (2015). Sui Dialect Research.</li>
<li> Cihui (1964). Wénzì Gǎigé.</li>
<li> Sun (1991). Zhōngguó Shèhuì Kēxué.</li>
<li> Allen (2007). Bai Dialect Survey.</li>
</ul></script></section><section data-markdown><script type="text/template"><h3>Thank you for the attention!</h3>

<p>CALC members:</p>
<ul>                        
<li> Dr. Johann-Mattis List (Group Leader)</li>    
<li> Dr. Yunfan Lai (Post-Doc)</li>
<li> Dr. Tiago Tresoldi (Post-Doc)</li>
<li> Nathanael E.Schweikhard (Doctoral student)</li>
<li> Mei-Shin Wu (Doctoral student)</li>
</ul>

<aside class="notes"><p>Thank you for your attentions. And I&#39;d like to thank all my colleagues for the help, especially Dr. Johann-Mattis List, our group leader. And I look forward to your input.</p>
</aside></script></section></section></div>

      <footer id="footer">
        <p>Mei-Shin Wu | wu@shh.mpg.de</p>
      </footer>
    </div>

    <script src="./lib/js/head.min.js"></script>
    <script src="./js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }
      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './plugin/zoom-js/zoom.js', async: true },
        { src: './plugin/notes/notes.js', async: true },
        { src: './plugin/math/math.js', async: true },
        { src: './plugin/reveald3/reveald3.js' }
      ];
      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps
      };
      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};
      var options = extend(defaultOptions, {}, queryOptions);
    </script>

    <script src="./_assets/jquery.js"></script>
    <script src="./_assets/additional.js"></script>

    <script>
      Reveal.initialize(options);
    </script>

  </body>

</html>
